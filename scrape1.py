import requests
import json
import time
from bs4 import BeautifulSoup

BASE_URL = "https://support.mehp.world"
TAG_API = f"{BASE_URL}/api-client/v1/article/tag"
LIST_API = f"{BASE_URL}/api-client/v1/tag/article/list"
DETAIL_API = f"{BASE_URL}/api-client/v1/article/info"

headers = {
    "User-Agent": "Mozilla/5.0",
    "Content-Type": "application/json"
}


def get_menu_ids():
    """è·å–æ‰€æœ‰äºŒçº§èœå•çš„ ID"""
    resp = requests.get(TAG_API, headers=headers)
    resp.raise_for_status()
    data = resp.json()
    ids = [str(item["blog_id"]) for item in data.get("data", [])]
    return ids


def get_article_ids_by_menu(menu_id, lang_name="zh_TW"):
    """æ ¹æ®èœå• ID è·å–æ–‡ç«  ID åˆ—è¡¨"""
    page = 1
    page_size = 100
    article_ids = []

    while True:
        payload = {"blog_id": int(menu_id), "headerType": "json", "lang_name": lang_name}

        resp = requests.post(LIST_API, headers=headers, json=payload)
        resp.raise_for_status()
        data = resp.json()
        items = data.get("data", {}).get("list", [])
        if not items:
            break

        for item in items:
            article_data = item.get("article", {}).get("rows", [])
            for article in article_data:
                aid = article.get("aid")
                if aid:
                    article_ids.append(aid)
        if len(items) < page_size:
            break
        page += 1
        time.sleep(0.3)

    return article_ids


def get_article_detail(article_id, lang_name="zh_TW"):
    """è·å–æ–‡ç« å†…å®¹"""
    payload = {"aid": int(article_id), "headerType": "json", "lang_name": lang_name}
    resp = requests.post(DETAIL_API, headers=headers, json=payload)
    resp.raise_for_status()
    data = resp.json()["data"]

    title = data["title"]
    html_content = data["content"]
    lang_name = data["lang_name"]

    # å°† HTML è½¬ä¸ºçº¯æ–‡æœ¬
    soup = BeautifulSoup(html_content, "html.parser")
    text = soup.get_text(separator="\n", strip=True)

    return {
        "id": article_id,
        "title": title,
        "content": text,
        "lang_name": lang_name
    }


def crawl_all():
    # æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
    languages = ["en", "es_ES", "ko_KR", "pt_PT", "th_TH", "tr_TR", "vi_VN", "zh_TW"]
    
    all_data = []
    menu_ids = get_menu_ids()
    print(f"å…±å‘ç° {len(menu_ids)} ä¸ªäºŒçº§èœå•")

    for lang_name in languages:
        print(f"æ­£åœ¨æŠ“å–è¯­è¨€: {lang_name}")
        for menu_id in menu_ids:
            print(f"ğŸ“‚ å¤„ç†èœå• ID: {menu_id}")
            article_ids = get_article_ids_by_menu(menu_id, lang_name)
            print(f"  - å‘ç°æ–‡ç«  {len(article_ids)} ç¯‡")

            for aid in article_ids:
                try:
                    detail = get_article_detail(aid, lang_name)
                    print(f"    âœ“ æŠ“å–æ–‡ç«  {aid}: {detail['title'][:30]}...")
                    all_data.append(detail)
                    time.sleep(0.5)
                except Exception as e:
                    print(f"    âœ— æŠ“å–å¤±è´¥ {aid}: {e}")

    return all_data


if __name__ == "__main__":
    articles = crawl_all()
    with open("mehp_articles.json", "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… å…±ä¿å­˜ {len(articles)} ç¯‡æ–‡ç« åˆ° mehp_articles.json")
